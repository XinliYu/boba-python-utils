import logging
from argparse import ArgumentParser
from functools import partial
from os import path
from typing import Union, Iterable

from boba_python_utils.general_utils.arg_utility.arg_naming import get_arg_name
from boba_python_utils.general_utils.arg_utility.arg_parse import get_parsed_args, ArgInfo, ARG_INFO
from boba_python_utils.general_utils.arg_utility.misc import update_args
from boba_python_utils.general_utils.argex import apply_arg_combo
from boba_python_utils.common_utils.attr_helper import setattr_if_none_or_empty_
from boba_python_utils.common_utils.slot_tuple import NamedTuple
from boba_python_utils.common_utils.typing_helper import solve_key_value_pairs
from boba_python_utils.io_utils.json_io import write_json
from boba_python_utils.general_utils.logging_utility import LoggableBase, get_file_logger
from boba_python_utils.path_utils.path_join import join_
from boba_python_utils.path_utils.path_string_operations import abspath_, path_or_name_with_timestamp
from boba_python_utils.path_utils.path_listing import get_paths_by_pattern
from boba_python_utils.path_utils.common import ensure_parent_dir_existence


class ExpArgInfo(NamedTuple):
    """
    Namedtuple for experiment argument definition.
    """
    __slots__ = ArgInfo.__slots__ + ('affect_batch', 'affect_vocab', 'could_be_path')

    def __init__(
            self,
            full_name='',
            short_name='',
            default_value=None,
            description='',
            converter=None,
            affect_batch=False,
            affect_vocab=False,
            could_be_path=False
    ):
        self.full_name = full_name
        self.short_name = short_name
        self.default_value = default_value
        self.description = description
        self.converter = converter
        self.affect_batch = affect_batch
        self.affect_vocab = affect_vocab
        self.could_be_path = could_be_path
        super(ExpArgInfo, self).__init__()


def get_workspace_path(data_type: str,
                       dataset_name: str,
                       data_version_name: str,
                       target_file_or_dir: Union[str, None] = None,
                       workspace_root_path: str = '.',
                       subspace_name: Union[str, None] = None,
                       abs_path=True):
    """
    Defines a 6-level workspace structure. From top-down, they are
    1) workspace root -> 2) sub workspace (optional) -> 3) data type -> 4) data set -> 5) data split -> 6) file or dir (optional).

    The **workspace root** is the root dir of the entire workspace, whose path is specified by `workspace_root_path`;
    The **sub workspace** is an optional subdir immediately under the workspace root, whose name is specified by `subspace_name`;
        a typical use of sub workspaces is to serve as the workspaces for the same experiment over different versions of data;
    The **data type** is a subdir immediately under a workspace (the workspace root or a sub workspace), separating different types of experiment data;
        for example we could have six types 'meta_data', 'source_data', 'datasets', 'results', 'analysis', 'logs';
        'meta_data': data shared by all experiments;
        'source_data': the original data from which the data sets can be built
        'datasets': the data after processing, the data which the experiment code can directly apply on;
        'results': the experiment results;
        'analysis': any analysis results, e.g. analysis for the source data, or analysis for the results, or plot, etc;
        'logs': experiment code logs generated by the logging modules.
    The **data set** is a subdir immediately under the 'data-type' level dir, which saves data of the particular type related to a data set.
    The **data split** is a subdir immediately under a 'data-set' level dir, which saves a split of the data set;
        typical data splits are run/test/validation splits;
        for data with timestamps, the data splits can also be months, so that we run a model with previous months and test on the next few months.
    Finally, the optional **target file or directory**, the name of the actually file or directory to look for; if this is not specified, then we look for the path to the data-split level directory.
    
    Args:
        data_type: the data-type level directory name; e.g. 'meta_data', 'source_data', 'datasets', 'results', 'analysis', 'logs', etc.
        dataset_name: the data set (directory) name.
        data_version_name: the data split (directory) name; e.g. 'run', 'test', 'val', etc.
        target_file_or_dir: the name of the target file or directory under the data split directory; if looking for the path the the data-split level directory, then leave this parameter as `None`.
        workspace_root_path: the path to the workspace dir.
        subspace_name: the name for the sub-workspace; if a sub-workspace is not needed, leave this parameter as `None`.
        abs_path: `True` if an absolute path should always be returned; otherwise `False`.
    Returns:
        the path to the target file or directory.

    """
    assert workspace_root_path
    assert data_type
    path_str = path.join(*(x for x in (workspace_root_path, subspace_name, dataset_name, data_type, data_version_name, target_file_or_dir) if x))
    return abspath_(path_str) if (abs_path and '://' not in path_str) else path_str


def get_input_path(
        args,
        data_type: Union[str, Iterable[str]],
        target: Union[str, Iterable[str]] = '',
        return_first_only=False,
        local=False,
        no_version_data_types=None,
        **kwargs
):
    """
    Gets input path(s) for the specified types of data in the workspace.
    This function assumes the workspace directory is structured according to `get_workspace_path`.

    Multiple arguments can be specified in `args` or `kwargs`
    for each of `get_workspace_path`'s parameter,
    and this function will either return all paths obtained
    by applying combinations of these arguments,
    or only one path by applying the first combination of these arguments,
    depending on `return_first_only`.

    Args:
        args: the arguments object containing arguments for `get_workspace_path`;
            the `args.data_type` argument in this object will be ignored,
            and instead we use the function argument `data_type`.
            Multiple arguments can be specified,
            for example, `args.dataset_name` can be a list of names;
            arguments of the same name in `kwargs` will override the arguments in `args`.
        data_type: specifies one or more data types, and we retrieve the input paths for them;
            multiple data types can be specified.
        target: specified any target file or subdirectory inside the data directory.
        return_first_only: True to return only one path by applying the first combination
            of arguments from `args` and `kwargs`.
        local: True to use `args.local_workspace_root` as the workspace root;
            otherwise use `args.workspace_root`;
            `args.local_workspace_root` should be guaranteed to be a local path, while
            `args.workspace_root` can be a remote path.
        **kwargs: overrides arguments in args, or supplements additional arguments to args.

    Returns: input path(s) for the specified types of data in the workspace.

    """
    args = update_args(args, **kwargs)
    workspace_root = kwargs.get('workspace_root', args.local_workspace_root if local else args.workspace_root)

    return apply_arg_combo(get_workspace_path,
                           data_type=data_type,
                           dataset_name=args.dataset,
                           data_version_name=(
                               args.data_version
                               if (
                                       (no_version_data_types is None)
                                       or (data_type not in no_version_data_types))
                               else None
                           ),
                           target_file_or_dir=target,
                           subspace_name=args.subspace,
                           workspace_root_path=workspace_root,
                           abs_path=False,
                           unpack_for_single_result=True,
                           return_first_only=return_first_only)


def get_output_path(
        args,
        data_type: Union[str, Iterable[str]],
        target: Union[str, Iterable[str]] = '',
        return_first_only=False,
        local=False,
        no_version_data_types=None,
        **kwargs
):
    """
    Gets input path(s) for the specified types of data in the workspace.

    Everything is the same as `get_input_path`,
    except for this function ensures the existence of the parent paths of the output paths.

    """
    output_path = get_input_path(
        args=args,
        data_type=data_type,
        target=target,
        return_first_only=return_first_only,
        local=local,
        no_version_data_types=no_version_data_types,
        **kwargs
    )
    if isinstance(output_path, str):
        if '://' not in output_path:
            output_path = ensure_parent_dir_existence(output_path)
    else:
        output_path = [
            ensure_parent_dir_existence(_output_path)
            for _output_path in output_path
        ]
    return output_path


DEFAULT_DIR_NAME_ARTIFACTS = 'artifacts'


class _ExperimentBase:
    def __init__(
            self,
            *arg_info_objs: ARG_INFO,
            dirs=None,
            no_version_dirs=None,
            artifact_dir_name: str = DEFAULT_DIR_NAME_ARTIFACTS,
            artifact_family=None,
            artifact_name=None,
            artifact_version=None,
            preset_root: str = None,
            preset: [dict, str] = None,
            expname_args=('mode',),
            expname: str = None,
            expname_prefix: str = None,
            expname_suffix: str = None,
            batchname_args=('batchsize',),
            batchname: str = None,
            batchname_prefix: str = None,
            batchname_suffix: str = None,
            arg_parser: ArgumentParser = None,
            **kwargs
    ):
        if dirs:
            dirs_setup = tuple(solve_key_value_pairs(
                dirs,
                parse_seq_as_alternating_key_value=False
            ))
            for dir_name_field, dir_name in dirs_setup:
                get_input_path_method_key = f'input_path_from_{dir_name_field}'
                get_output_path_method_key = f'output_path_to_{dir_name_field}'
                assert not hasattr(self, dir_name_field)
                # region 
                # ! no longer make the name existence checks; 
                # we might need to declare them for IDE auto-completion
                # msgex.assert_name_not_defined(obj=self, member_name=get_input_path_method_key)
                # msgex.assert_name_not_defined(obj=self, member_name=get_output_path_method_key)
                # endregion
                setattr(self, dir_name_field, dir_name)
                setattr(self, get_input_path_method_key, partial(self._get_input_path, data_type=getattr(self, dir_name_field)))
                setattr(self, get_output_path_method_key, partial(self._get_output_path, data_type=getattr(self, dir_name_field)))

        self.no_version_dirs = no_version_dirs
        self.artifact_dir_name = artifact_dir_name
        self.artifact_family = artifact_family
        self.artifact_name = artifact_name
        self.artifact_version = artifact_version
        self.args, self.terminal_set_arg_names = get_parsed_args(
            *(
                    (
                        ('workspace_root', '.', 'The root path to the experiment workspace. Multiple workspace roots are possibly OK but discouraged and not supported.'),
                        ('subspace', '', 'The name of the sub-workspace to use. Multiple sub-workspaces are possibly OK but discouraged and not supported.'),
                        ('dataset', 'main_data', 'The name(s) of the data set(s) to use in the experiment; multiple data sets are supported.'),
                        ('data_version', '', 'The name(s) for the experiment data split(s), e.g. the training data for modeling-based experiments; multiple data splits are supported.'),
                        ('mode', 'default', 'The experiment mode. Interpretation of this parameter is dependent on the specific experiment.'),
                        ('expname', expname, 'Specifies the name for the experiment.'),
                    ) + arg_info_objs
            ),
            preset_root=preset_root,
            preset=preset,
            return_seen_args=True,
            arg_parser=arg_parser,
            **kwargs
        )

        if hasattr(self.args, 'local_workspace_root'):
            if dirs:
                for dir_name_field, dir_name in dirs_setup:
                    get_local_input_path_method_key = f'local_input_path_from_{dir_name_field}'
                    get_local_output_path_method_key = f'local_output_path_to_{dir_name_field}'
                    # region
                    # ! no longer make the name existence checks; 
                    # we might need to declare them for IDE auto-completion
                    # msgex.assert_name_not_defined(obj=self, member_name=get_local_input_path_method_key)
                    # msgex.assert_name_not_defined(obj=self, member_name=get_local_output_path_method_key)
                    # endregion
                    setattr(self, get_local_input_path_method_key, partial(self._get_local_input_path, data_type=getattr(self, dir_name_field)))
                    setattr(self, get_local_output_path_method_key, partial(self._get_local_output_path, data_type=getattr(self, dir_name_field)))

        self.name = get_arg_name(args=self.args,
                                 active_argname_info=expname_args,
                                 name=getattr(self.args, 'expname', expname),
                                 name_prefix=expname_prefix,
                                 name_suffix=expname_suffix)
        self.batchname = get_arg_name(args=self.args,
                                      active_argname_info=batchname_args,
                                      name=batchname,
                                      name_prefix=batchname_prefix,
                                      name_suffix=batchname_suffix)

    def _artifacts_path(
            self,
            root_path,
            artifact_type,
            artifact_family=None,
            artifact_name=None,
            version=None
    ):
        artifact_family = artifact_family or self.artifact_family
        artifact_name = artifact_name or self.artifact_name
        version = version or self.artifact_version
        return path.join(
            root_path,
            self.artifact_dir_name,
            artifact_type,
            join_(
                artifact_family, artifact_name, version
            )
        )

    def artifacts_path(
            self,
            artifact_type,
            artifact_family=None,
            artifact_name=None,
            version=None
    ):
        return self._artifacts_path(
            root_path=self.args.workspace_root,
            artifact_type=artifact_type,
            artifact_family=artifact_family,
            artifact_name=artifact_name,
            version=version
        )

    def local_artifacts_path(
            self,
            artifact_type,
            artifact_family=None,
            artifact_name=None,
            version=None
    ):
        return self._artifacts_path(
            root_path=self.args.local_workspace_root,
            artifact_type=artifact_type,
            artifact_family=artifact_family,
            artifact_name=artifact_name,
            version=version
        )

    def _get_input_path(self, target='', data_type='', **kwargs):
        return get_input_path(args=self.args, data_type=data_type, target=target, no_version_data_types=self.no_version_dirs, **kwargs)

    def _get_output_path(self, target='', data_type='', **kwargs):
        return get_output_path(args=self.args, data_type=data_type, target=target, no_version_data_types=self.no_version_dirs, **kwargs)

    def _get_local_input_path(self, target='', data_type='', **kwargs):
        return get_input_path(args=self.args, data_type=data_type, target=target, no_version_data_types=self.no_version_dirs, local=True, **kwargs)

    def _get_local_output_path(self, target='', data_type='', **kwargs):
        return get_output_path(args=self.args, data_type=data_type, target=target, no_version_data_types=self.no_version_dirs, local=True, **kwargs)

    def unprefix_args(self, prefix):
        len_prefix = len(prefix)
        return {k[len_prefix:]: v for k, v in self.args.__dict__.items() if k.startswith(prefix)}

    def get_arg_names(self, prefix):
        return tuple(k for k in self.args.__dict__ if k.startswith(prefix))


# region general common arguments
_ARG_DEFAULT_TEST_SPLIT_NAME = 'test'
_ARG_DEFAULT_VAL_SPLIT_NAME = 'val'
_ARG_DEFAULT_DATA_MAX_READ = 100000000000
_ARG_DEFAULT_MULTIPROCESSING = 1
_ARG_DEFAULT_VERBOSE = True
_ARG_DEFAULT_RAND_SEED = 0
_ARG_DEFAULT_VAL_PROB = 0.1
_ARG_DEFAULT_VAL_MAX_SIZE = 5000
_ARG_DEFAULT_TEST_PROB = 0.1
_ARG_DEFAULT_TEST_MAX_SIZE = 5000
_ARG_DEFAULT_NO_FILE_CACHE = False
_ARG_DEFAULT_NO_MEMORY_CACHE = False
_ARG_DEFAULT_COMPRESS_CACHE = False
_ARG_DEFAULT_SHUFFLE_CACHE = False
_ARG_DEFAULT_LESS_EVAL_WHILE_TRAINING = True
_ARG_DEFAULT_MAX_ITER = 0
_ARG_DEFAULT_PRE_TEST_INTERVAL = 0
_ARG_DEFAULT_STAT_HARDWARE_USAGE = False
_ARG_DEFAULT_RUN_MODE = 'run'
_ARG_DEFAULT_DEBUG_MODE = False

ARG_SETUP_ESSENTIAL = (
    ExpArgInfo(full_name='verbose', default_value=_ARG_DEFAULT_VERBOSE, description='Printout/Log internal messages when applicable.'),
    ExpArgInfo(full_name='random_seed', default_value=_ARG_DEFAULT_RAND_SEED, description='The random seed for reproducible experiments.')
)
ARG_SETUP_GENERAL = (
    ExpArgInfo(full_name='test_split', default_value=_ARG_DEFAULT_TEST_SPLIT_NAME, description='The name(s) for the test data split(s); multiple data splits are supported.'),
    ExpArgInfo(full_name='val_split', default_value=_ARG_DEFAULT_VAL_SPLIT_NAME, description='The name(s) for the validation data split(s); multiple data splits are supported.'),
    ExpArgInfo(full_name='data_max_read', default_value=_ARG_DEFAULT_DATA_MAX_READ, description='Limit the maximum number of loaded data entries for experiment.', affect_batch=True, affect_vocab=True),
    ExpArgInfo(full_name='multi_processing', default_value=_ARG_DEFAULT_MULTIPROCESSING, description='The number of parallel processes for parallel processing when applicable.'),
    ExpArgInfo(full_name='test_ratio', default_value=_ARG_DEFAULT_TEST_PROB, description='The sampling ratio (probability) for the test set.'),  # TODO obsolete
    ExpArgInfo(full_name='val_ratio', default_value=_ARG_DEFAULT_VAL_PROB, description='The sampling ratio (probability) for the validation set.'),  # TODO obsolete
    ExpArgInfo(full_name='test_max_size', default_value=_ARG_DEFAULT_TEST_MAX_SIZE, description='The maximum size of the test set.'),  # TODO obsolete
    ExpArgInfo(full_name='val_max_size', default_value=_ARG_DEFAULT_VAL_MAX_SIZE, description='The maximum size of the validation set.'),  # TODO obsolete
    ExpArgInfo(full_name='init', default_value='', description='The initialization; interpretation dependent on the experiment.', could_be_path=True),
    ExpArgInfo(full_name='no_file_cache', default_value=_ARG_DEFAULT_NO_FILE_CACHE, description='Disables any file-based caching mechanism when applicable.'),
    ExpArgInfo(full_name='no_mem_cache', default_value=_ARG_DEFAULT_NO_MEMORY_CACHE, description='Disables any memory-based caching mechanism when applicable.'),
    ExpArgInfo(full_name='compress_cache', default_value=_ARG_DEFAULT_COMPRESS_CACHE, description='Compressed the cached data when applicable.'),
    ExpArgInfo(full_name='shuffle_cache', default_value=_ARG_DEFAULT_SHUFFLE_CACHE, description='The cached data items are shuffled when applicable.'),
    ExpArgInfo(full_name='less_eval_while_training', default_value=_ARG_DEFAULT_LESS_EVAL_WHILE_TRAINING, description='Serves as a signal to the experiment pipeline to fields_sub the cost of evaluation during training when applicable (e.g. no detailed model output dump).'),
    ExpArgInfo(full_name='pre_test_interval', default_value=_ARG_DEFAULT_PRE_TEST_INTERVAL, description='Sets a positive integer to enable premature test during training (i.e. evaluate on the test set before fully training the model for all epochs); '
                                                                                                        'the integer is the number of epochs between two tests. '
                                                                                                        'Set this number to 0 to disable premature tests.'),
    ExpArgInfo(full_name='max_iter', default_value=_ARG_DEFAULT_MAX_ITER, description='The maximum number of iterations. The interpretation of this number is model/experiment dependent.'),
    ExpArgInfo(full_name='stat_hardware_usage', default_value=_ARG_DEFAULT_STAT_HARDWARE_USAGE, description='The maximum number of iterations. The interpretation of this number is model/experiment dependent.'),
    ExpArgInfo(full_name='debug_mode', default_value=_ARG_DEFAULT_DEBUG_MODE, description='true to enable the debug mode; the interpretation of this argument is dependent on the experiment.'),
    ExpArgInfo(full_name='run_mode', default_value=_ARG_DEFAULT_RUN_MODE, description='The default mode for the experiment pipeline, e.g. training, evaluation, or embedding dump, etc.; the interpretation of this argument is dependent on the pipeline.')
)
EXP_NAME_ARGS_GENERAL = ('random_seed', 'data_max_read')
EXP_BATCH_NAME_ARGS_GENERAL = EXP_NAME_ARGS_GENERAL

# endregion

# region workspace arguments
ARG_WORKSPACE_OVERRIDES = (
    ExpArgInfo(full_name='result_path', description='The path to the result directory. Specify this to override the workspace result path.', could_be_path=True),
    ExpArgInfo(full_name='runtime_path', description='The path to the current runtime directory. Specify this to override the workspace runtime path.', could_be_path=True),
    ExpArgInfo(full_name='log_path', description='The path to the log directory. Specify this to override the workspace log path.', could_be_path=True),
    ExpArgInfo(full_name='cache_path', description='The path to the cache directory. Specify this to override the workspace cache path.', could_be_path=True),
    ExpArgInfo(full_name='data_path', description='The path to the cache directory. Specify this to override the workspace data path.', could_be_path=True),
    ExpArgInfo(full_name='test_set_path', description='The path to the test set directory. Specify this to override the workspace test set path.', could_be_path=True),
    ExpArgInfo(full_name='val_set_path', description='The path to the validation set directory. Specify this to override the workspace validation set path.', could_be_path=True)
)


# endregion

class ExperimentBase(_ExperimentBase, LoggableBase):
    # region common path functions

    def source_data_path(self, target='', output=False, **kwargs):
        """
        Gets the path to the particular source data specified by `target`.
        :param target:
        :param output:
        :param kwargs:
        :return:
        """
        return self.output_path_to_source_data(target=target, **kwargs) if output else self.input_path_from_source_data(target=target, **kwargs)

    def datasets_path(self, target='', output=False, **kwargs):
        return self.output_path_to_datasets(target=target, **kwargs) if output else self.input_path_from_datasets(target=target, **kwargs)

    def analysis_path(self, target='', output=False, **kwargs):
        return self.output_path_to_analysis(target=target, **kwargs) if output else self.input_path_from_analysis(target=target, **kwargs)

    def results_path(self, target='', output=False, **kwargs):
        return self.output_path_to_results(target=target, **kwargs) if output else self.input_path_from_results(target=target, **kwargs)

    def input_path_from_source_data(self, target='', **kwargs):
        ...

    def input_path_from_datasets(self, target='', **kwargs):
        ...

    def input_path_from_analysis(self, target='', **kwargs):
        ...

    def input_path_from_results(self, target='', **kwargs):
        ...

    def output_path_to_artifacts(self, target='', **kwargs) -> str:
        ...

    def output_path_to_source_data(self, target='', **kwargs) -> str:
        ...

    def output_path_to_datasets(self, target='', **kwargs) -> str:
        ...

    def output_path_to_analysis(self, target='', **kwargs) -> str:
        ...

    def output_path_to_results(self, target='', **kwargs) -> str:
        ...

    def local_input_path_from_source_data(self, target='', **kwargs):
        ...

    def local_input_path_from_datasets(self, target='', **kwargs):
        ...

    def local_input_path_from_analysis(self, target='', **kwargs):
        ...

    def local_input_path_from_results(self, target='', **kwargs):
        ...

    def local_output_path_to_artifacts(self, target='', **kwargs) -> str:
        ...

    def local_output_path_to_source_data(self, target='', **kwargs) -> str:
        ...

    def local_output_path_to_datasets(self, target='', **kwargs) -> str:
        ...

    def local_output_path_to_analysis(self, target='', **kwargs) -> str:
        ...

    def local_output_path_to_results(self, target='', **kwargs) -> str:
        ...



    # endregion

    def __init__(
            self,
            *arg_info_objs: ARG_INFO,
            dirs=('source_data', 'datasets', 'meta_data', 'results', 'analysis'),
            no_version_dirs=('source_data', 'meta_data'),
            preset_root: str = None,
            preset: [dict, str] = None,
            default_workspace_root='.',
            general_args=True,
            workspace_override_args=True,
            simple_experiment=False,
            expname_args=(),
            expname_prefix='',
            expname_suffix='',
            batchname_args=(),
            batchname_prefix='',
            batchname_suffix='',
            arg_parser: ArgumentParser = None,
            **kwargs
    ):
        """
        A base class for workspace and configuration management.

        Workspace.
        ----------
        This class automatically creates directories under the workspace root folder, and provides methods to access the specified data folders or files.
        We recommend five workspace data types, and 6-level workspace structure.

        The five workspace data types include:
        1) **artifacts**: store the configs, meta data, etc. that are meant to be shared by all experiments.
        2) **source data**: store the initial data; the source data is typically not cleaned or filtered and contains extra information.
        3) **datasets**: save processed data directly used for the experiments; can hold data splits like the training data, validation data and test data.
        4) **results**: save the experiment results, typically the results of the main experiment pipeline.
        5) **analysis**: save data exploration results, further analysis on the experiment results or other miscellaneous results; an analysis is typically done by quick scripts that explore the data or main experiment results.

        The six-level workspace structure are: 1) workspace root -> 2) sub workspace (optional) -> 3) data type -> 4) data set -> 5) data split -> 6) file or dir (optional).
        See the `get_workspace_path` function for details.

        Argument Parsing.
        -----------------
        This class parses the terminal arguments using `utix.argex.get_parsed_args` function.

        Args:
            arg_info_objs: argument definition objects; this is passed to `get_parsed_args` for parsing terminal arguments.
            dir_meta_data: the directory name for the metadata; specify `None` to indicate there is no need for a metadata directory.
            dir_source_data: the directory name for the source data; specify `None` to indicate there is no need for a source-data directory.
            dir_datasets: the directory name for the datasets; specify `None` to indicate there is no need for a dataset directory;
            dir_results: the directory name for the experiment results; specify `None` to indicate there is no need for a result directory;
            dir_analysis: the directory name for the data and result analysis; specify `None` to indicate there is no need for an analysis directory;
            preset_root: the path to the directory that stores presets of arguments; this is passed to the `utix.argex.get_parsed_args` function.
            preset: the path/name of the argument preset file relative to `arg_preset_root`; this is passed to the `utix.argex.get_parsed_args` function.
            default_workspace_root: the path to the default workspace root folder.
            general_args: `True` to add build-in common argument definitions to the `arg_info_objs`.
            workspace_override_args: `True` to add build-in specialized workspace argument definitions to the `arg_info_objs`.
            deep_learning_args: `True` to add build-in deep-learning argument definitions to the `arg_info_objs`.
            nlp_args: `True` to add build-in NLP argument definitions to the `arg_info_objs`.
            simple_experiment: a convenient argument; set this to `True` if the experiment is a simple script that only uses specified arguments in the `arg_info_objs`; otherwise, `False`.
            expname_args: specify the names of important arguments, and these argument names and values are used to construct the experiment name so that the experiment name provides hint to what arguments are used for the experiment.
            expname_prefix: the prefix for the experiment name.
            expname_suffix: the suffix for the experiment name.
            kwargs: other arguments.
        """
        arg_info_objs += ARG_SETUP_ESSENTIAL
        if not simple_experiment:
            arg_info_objs += (
                    (ARG_SETUP_GENERAL if general_args else ()) +
                    (ARG_WORKSPACE_OVERRIDES if workspace_override_args else ())
            )
            expname_args += (EXP_NAME_ARGS_GENERAL if general_args else ())
            batchname_args += (EXP_BATCH_NAME_ARGS_GENERAL if general_args else ())
        _ExperimentBase.__init__(self,
                                 *arg_info_objs,
                                 dirs=dirs,
                                 no_version_dirs=no_version_dirs,
                                 preset_root=preset_root,
                                 preset=preset,
                                 default_workspace_root=default_workspace_root,
                                 expname_args=expname_args,
                                 expname_prefix=expname_prefix,
                                 expname_suffix=expname_suffix,
                                 batchname_args=batchname_args,
                                 batchname_prefix=batchname_prefix,
                                 batchname_suffix=batchname_suffix,
                                 arg_parser=arg_parser,
                                 **kwargs)

        # region adds convenient fields to `args`
        args = self.args
        recognized_path_prefixes = {
            '$results/': self.input_path_from_results,
            '$data/': self.input_path_from_datasets,
        }
        for arg_info_obj in arg_info_objs:
            if isinstance(arg_info_obj, ExpArgInfo):
                if arg_info_obj.could_be_path:
                    argval = getattr(args, arg_info_obj.full_name)
                    if argval:
                        for prefix, path_func in recognized_path_prefixes.items():  # TODO make this a formal mechanism
                            if argval.startswith(prefix):
                                setattr(args, arg_info_obj.full_name, path_func(target=argval[len(prefix):]))

        if not simple_experiment:
            setattr_if_none_or_empty_(args, 'result_path', lambda: self.output_path_to_results(target=self.name))
            setattr_if_none_or_empty_(args, 'runtime_path', lambda: path_or_name_with_timestamp(path.join(args.result_path, 'run')))
            setattr_if_none_or_empty_(args, 'debug_path', lambda: path.join(args.runtime_path, 'debug'))
            setattr_if_none_or_empty_(args, 'cache_path', lambda: path.join(self.output_path_to_results(), '_cache', self.batchname))
            setattr_if_none_or_empty_(args, 'vocab_path', lambda: path.join(self.output_path_to_results(), '_vocab'))
            setattr_if_none_or_empty_(args, 'log_path', lambda: path.join(args.runtime_path, '_logs'))
            LoggableBase.__init__(self, path.join(args.log_path, 'main.log'), print_out=kwargs.get('verbose'))
            self.info_message('experiment result path', args.result_path)
            self.info_message('experiment runtime path', args.runtime_path)
            self.info_message('experiment log path', args.log_path)
            self.info_message('experiment cache path', args.cache_path)
            setattr_if_none_or_empty_(args, 'data_path', lambda: self.input_path_from_datasets())
            self.info_message('experiment data path', args.input_path_traffic)

            if general_args:
                setattr_if_none_or_empty_(args, 'test_set_path', lambda: self.input_path_from_test_sets())
                self.info_message('experiment test set path', args.test_set_path)
                setattr_if_none_or_empty_(args, 'val_set_path', lambda: self.input_path_from_val_sets())
                self.info_message('experiment validation set path', args.val_set_path)

            # region saving the arguments
            write_json(args, path.join(args.runtime_path, self.name + '.json'), indent=2)
            # endregion
        # endregion

    def get_logger(
            self,
            log_name: str,
            logging_level=logging.DEBUG,
            log_format="%(asctime)s - %(process)d - %(name)s - %(levelname)s - %(message)s",
            append=False,
            file_ext='log'
    ):
        if hasattr(self.args, 'log_path'):
            return get_file_logger(
                name=log_name,
                log_dir_path=self.args.log_path,
                logging_level=logging_level,
                log_format=log_format,
                append=append,
                file_ext=file_ext
            )

    def get_init_path(self, init_ext='.th'):
        """
        Gets the initialization path. This method tries to search for an initialization path based on the following rules:
        1) if '--init' argument is specified, then
            returns the init argument if it points to an existing path;
            otherwise, searching the directory of the argument present, or the experiment's result path, to see if the '--init' argument exists_path as a file or sub-directory, and returns the joined path if so.
        2) otherwise, checks the directory of the preset for a file with extension name specified by `init_ext`;
            returns the file path if there is just a single path;
            otherwise, checks for a file with the main name 'best' and the extension name specified by `init_ext`, and returns the path to that file if it exists_path;
            otherwise, returns a file with the largest alphabetic order in the directory with the extension name.
        """
        if hasattr(self.args, 'init'):
            init = self.args.init
            if init:
                if path.exists(init):
                    return init
                if hasattr(self.args, 'preset'):  # checks if `init` is in the `preset`'s directory
                    _init = path.join(path.dirname(self.args.preset), init)
                    if path.exists(_init):
                        return _init
                _init = path.join(self.results_path(), init)
                if path.exists(_init):
                    return _init
        elif hasattr(self.args, 'preset'):
            if init_ext[0] != '.':
                init_ext = '.' + init_ext
            init_dir = path.dirname(self.args.preset)
            possible_inits = get_paths_by_pattern(dir_or_dirs=init_dir, pattern='*' + init_ext, full_path=True, recursive=False)
            if len(possible_inits) == 1:
                return possible_inits[0]
            elif possible_inits:
                init = path.join(init_dir, 'best' + init_ext)
                if path.exists(init):
                    return init
                possible_inits.sort(reverse=True)
                return possible_inits[0]

    def output_path_to_default_log_file(self):
        return self.output_path_to_logs(target=f'{self.name}.log')

    def input_path_from_test_sets(self, target: Union[str, Iterable] = '', multi_path=False, **kwargs) -> str:
        return get_input_path(self.args, data_type=self.dir_datasets, data_version=self.args.test_split if hasattr(self.args, 'test_split') else 'test', target=target, multipath=multi_path, **kwargs)

    def input_path_from_val_sets(self, target: Union[str, Iterable] = '', multi_path=False, **kwargs) -> str:
        return get_input_path(self.args, data_type=self.dir_datasets, data_version=self.args.val_split if hasattr(self.args, 'val_split') else 'val', target=target, multipath=multi_path, **kwargs)

    def output_path_to_test_sets(self, target: Union[str, Iterable] = '', **kwargs) -> str:
        return get_output_path(self.args, data_type=self.dir_datasets, data_version=self.args.test_split if hasattr(self.args, 'test_split') else 'test', target=target, **kwargs)

    def output_path_to_val_sets(self, target: Union[str, Iterable] = '', **kwargs) -> str:
        return get_output_path(self.args, data_type=self.dir_datasets, data_version=self.args.val_split if hasattr(self.args, 'val_split') else 'val', target=target, **kwargs)
